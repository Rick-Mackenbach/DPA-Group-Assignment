{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code to improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "#print('After running first results, the following sentence was found : ')\n",
    "##line 496 is the weird one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.save(\"descriptions_cleaned_array.npy\",dataSet)\n",
    "dataSet = numpy.load(\"descriptions_cleaned_array.npy\")\n",
    "#dataSet = numpy.load(\"coco_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and neatly fit into an array. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  1479\n",
      "The dimension of our dataset are:  (1479,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " jug ears mustache and beard and long sideburns stylish hair no laugh lines eyes are clear no drugs or alcohol confident a little overweight from double chin\n"
     ]
    }
   ],
   "source": [
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be trained to obtain accuracy measures of classification. Firstly, countVectorizer by scikitLearn (which counts all the instances of words) will run on our cleaned dataset. Afterwards TfIdf will run, in order the have the Term frequency, inverse document frequency (which will essentially remove non-informative words suchs as: 'the', 'and', 'a'). Scikit-learn provides a neat function to do this in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 228)\t0.0999230235196548\n",
      "  (0, 2737)\t0.16083047896043273\n",
      "  (0, 1776)\t0.06254106376813398\n",
      "  (0, 2119)\t0.3161753448886334\n",
      "  (0, 1247)\t0.14464356086569088\n",
      "  (0, 2538)\t0.19064057405466286\n",
      "  (0, 419)\t0.18076247495377387\n",
      "  (0, 2292)\t0.1401975901762004\n",
      "  (0, 3445)\t0.21120111824703844\n",
      "  (0, 3749)\t0.2492839547283478\n",
      "  (0, 2604)\t0.3123113475808705\n",
      "  (0, 2188)\t0.2417063139284673\n",
      "  (0, 2262)\t0.264108063599575\n",
      "  (0, 1420)\t0.07045478691841404\n",
      "  (0, 286)\t0.11292286212515924\n",
      "  (0, 772)\t0.21289776024949109\n",
      "  (0, 1217)\t0.2324320311261342\n",
      "  (0, 2694)\t0.1235469355267728\n",
      "  (0, 180)\t0.27051486760744353\n",
      "  (0, 878)\t0.20649095624162253\n",
      "  (0, 2276)\t0.14173141580413545\n",
      "  (0, 1605)\t0.1515561961580996\n",
      "  (0, 1191)\t0.3161753448886334\n",
      "  (0, 740)\t0.1644468421153148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[1:2])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents will be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  66  406 1084   65 1453]\n",
      " [ 144 1372  797  548  555]\n",
      " [1173 1270 1050  523  342]\n",
      " ...\n",
      " [ 590  446 1170  205 1371]\n",
      " [ 406  178  831  966 1063]\n",
      " [1033 1143  540  499  279]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "cosineSimilaritySorted = numpy.argsort(cosineSimilarity, axis=1)\n",
    "top5Similar = cosineSimilaritySorted[:,-6:-1]\n",
    "\n",
    "print(top5Similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the cosine similarity\n",
    "\n",
    "Following the cosine metric, the 1159 sentence in our dataSet is closest to the 1251 sentence in our data set. Let's see what they both look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1159 in the dataSet: \n",
      "somewhat athletic fair skin average weight normal appearance no tattoos parents have a fair amount of money has sexually assaulted a female has never had a job\n",
      "\n",
      "Sentence 1251 in the dataSet: \n",
      "probably skinny they may be going a little bald possibly taller than average possibly hyper energetic they seem like they are a bit mess unkept\n"
     ]
    }
   ],
   "source": [
    "print('Sentence 1159 in the dataSet: ')\n",
    "print(dataSet[1159])\n",
    "print()\n",
    "print('Sentence 1251 in the dataSet: ')\n",
    "print(dataSet[1251])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Besides finding similar documents by cosine similarity, an implementation of KMeans clustering is done in the following code. This is more meaningful, since the information is known that there are 5 sentences that are equal to each other, therefore making the number of clusters to 296. Also, it allows for topic extraction, which can be interpreted as the most important words for each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = sklearn.cluster.KMeans(n_clusters=296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "KmeansFit = KMeans.fit(TfIdf_dataSet)\n",
    "labels = KMeans.predict(TfIdf_dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2912 2913 2914 2915 2916]\n",
      " [2898 2899 2900 2901 2902]\n",
      " [2878 2879 2880 2881 2882]\n",
      " ...\n",
      " [2905 2906 2907 2908 2909]\n",
      " [2911 2912 2913 2914 2915]\n",
      " [2889 2890 2891 2892 2893]]\n"
     ]
    }
   ],
   "source": [
    "a = numpy.argsort(KmeansFit.cluster_centers_, axis = 1)\n",
    "top5SimilarKMeans = a[:,1:6]\n",
    "print(top5SimilarKMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">I think we still need to order by the first column's value so that we get the lines in a row. Then we save as a CSV as noted in code line below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save results to results file\n",
    "numpy.savetxt(\"results.csv\",top5SimilarKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " denny\n",
      " complextion\n",
      " tint\n",
      " waitress\n",
      " 150\n",
      " mom\n",
      " 20\n",
      " early\n",
      " brown\n",
      " one\n",
      "Cluster 1:\n",
      " hang\n",
      " with\n",
      " light\n",
      " out\n",
      " likes\n",
      " friends\n",
      " skin\n",
      " brown\n",
      " weekend\n",
      " working\n",
      "Cluster 2:\n",
      " she\n",
      " her\n",
      " to\n",
      " weighing\n",
      " has\n",
      " everything\n",
      " wear\n",
      " keep\n",
      " active\n",
      " way\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = KmeansFit.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(3):\n",
    "    print (\"Cluster %d:\" % i,)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print (' %s' % terms[ind],)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "To actually get a better estimation of our text similarity, several tests are performed on a test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
