{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code to improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a young man holding an umbrella next to a herd of cattle' '']\n",
      "['a young boy barefoot holding an umbrella touching the horn of a cow' '']\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "descriptions2 = []\n",
    "\n",
    "with open('coco_val.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        descriptions2.append(text.split('\\n'))\n",
    "\n",
    "trainSet = numpy.array(descriptions2)\n",
    "f.close()\n",
    "\n",
    "print(trainSet[1])\n",
    "print(trainSet[2])\n",
    "print(trainSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned according to a list of lists. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  1480\n",
      "The dimension of our dataset are:  (1480,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " jug ears mustache and beard and long sideburns stylish hair no laugh lines eyes are clear no drugs or alcohol confident a little overweight from double chin\n"
     ]
    }
   ],
   "source": [
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be trained to obtain accuracy measures of classification. Firstly, countVectorizer by scikitLearn (which counts all the instances of words) will run on our cleaned dataset. Afterwards TfIdf will run, in order the have the Term frequency, inverse document frequency (which will essentially remove non-informative words suchs as: 'the', 'and', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 228)\t0.0999230235196548\n",
      "  (0, 2737)\t0.16083047896043273\n",
      "  (0, 1776)\t0.06254106376813398\n",
      "  (0, 2119)\t0.3161753448886334\n",
      "  (0, 1247)\t0.14464356086569088\n",
      "  (0, 2538)\t0.19064057405466286\n",
      "  (0, 419)\t0.18076247495377387\n",
      "  (0, 2292)\t0.1401975901762004\n",
      "  (0, 3445)\t0.21120111824703844\n",
      "  (0, 3749)\t0.2492839547283478\n",
      "  (0, 2604)\t0.3123113475808705\n",
      "  (0, 2188)\t0.2417063139284673\n",
      "  (0, 2262)\t0.264108063599575\n",
      "  (0, 1420)\t0.07045478691841404\n",
      "  (0, 286)\t0.11292286212515924\n",
      "  (0, 772)\t0.21289776024949109\n",
      "  (0, 1217)\t0.2324320311261342\n",
      "  (0, 2694)\t0.1235469355267728\n",
      "  (0, 180)\t0.27051486760744353\n",
      "  (0, 878)\t0.20649095624162253\n",
      "  (0, 2276)\t0.14173141580413545\n",
      "  (0, 1605)\t0.1515561961580996\n",
      "  (0, 1191)\t0.3161753448886334\n",
      "  (0, 740)\t0.1644468421153148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Now we can safely compute the distance between each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.0607874  0.01502397 ... 0.04558553 0.06715282 0.09317791]\n",
      " [0.0607874  1.         0.07117968 ... 0.06126724 0.0548427  0.0650882 ]\n",
      " [0.01502397 0.07117968 1.         ... 0.09200355 0.05217812 0.03591779]\n",
      " ...\n",
      " [0.04558553 0.06126724 0.09200355 ... 1.         0.06750285 0.05996048]\n",
      " [0.06715282 0.0548427  0.05217812 ... 0.06750285 1.         0.21531977]\n",
      " [0.09317791 0.0650882  0.03591779 ... 0.05996048 0.21531977 1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 1477, 1478, 1479])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "print(cosineSimilarity)\n",
    "numpy.argmax(cosineSimilarity, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = sklearn.cluster.KMeans(n_clusters=296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.19625855 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "KmeansFit = KMeans.fit(TfIdf_dataSet)\n",
    "labels = KMeans.predict(TfIdf_dataSet)\n",
    "C = KMeans.cluster_centers_\n",
    "print(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
