{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import string\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 3085)\t0.18016293756155294\n",
      "  (0, 1351)\t0.10858700066557808\n",
      "  (0, 3257)\t0.3001624823648848\n",
      "  (0, 2585)\t0.34043262183046635\n",
      "  (0, 2139)\t0.11306416211646501\n",
      "  (0, 4024)\t0.21854146699056293\n",
      "  (0, 1981)\t0.25581194154091114\n",
      "  (0, 3626)\t0.31679149916873117\n",
      "  (0, 1152)\t0.24315753998842082\n",
      "  (0, 4070)\t0.2684663430934014\n",
      "  (0, 1073)\t0.30413709761624086\n",
      "  (0, 2120)\t0.2684663430934014\n",
      "  (0, 1687)\t0.06619086894558779\n",
      "  (0, 1200)\t0.26383180239523174\n",
      "  (0, 1473)\t0.31679149916873117\n",
      "  (0, 1117)\t0.16480321090672406\n",
      "  (0, 4092)\t0.16167588241502212\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1480, 300)\n"
     ]
    }
   ],
   "source": [
    "LSA = TruncatedSVD(n_components=300)\n",
    "LSAData = LSA.fit_transform(TfIdf_dataSet)\n",
    "print(LSAData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[   0 1454   66 1085  406]\n",
      " [   1  556  549 1373  206]\n",
      " [   2  342  379  811    4]\n",
      " ...\n",
      " [1477 1372  530  392  681]\n",
      " [1478  967  669  706 1084]\n",
      " [1479 1341  352 1144  504]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(LSAData)\n",
    "numpy.fill_diagonal(cosineSimilarity,1.1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:5])\n",
    "print()\n",
    "print(top5similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "plain freckles short hair boring eyes likes earrings hair parted normally probably slightly overweight but not sure about that she likes twilight she isn t married she is sad she is slightly overweight\n"
     ]
    }
   ],
   "source": [
    "print(dataSet[0])\n",
    "print(dataSet[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savetxt(\"results4.csv\", top5similar.astype(int), fmt='%i', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
