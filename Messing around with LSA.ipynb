{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rickmackenbach/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rickmackenbach/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import string\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running first results, the following sentence was found : \n",
      "\n",
      "rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad\n",
      "\n",
      "\n",
      "Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.\n"
     ]
    }
   ],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "\n",
    "print('After running first results, the following sentence was found : ')\n",
    "print()\n",
    "print(dataSet[496])\n",
    "print()\n",
    "print()\n",
    "print('Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 3085)\t0.18016293756155294\n",
      "  (0, 1351)\t0.10858700066557808\n",
      "  (0, 3257)\t0.3001624823648848\n",
      "  (0, 2585)\t0.34043262183046635\n",
      "  (0, 2139)\t0.11306416211646501\n",
      "  (0, 4024)\t0.21854146699056293\n",
      "  (0, 1981)\t0.25581194154091114\n",
      "  (0, 3626)\t0.31679149916873117\n",
      "  (0, 1152)\t0.24315753998842082\n",
      "  (0, 4070)\t0.2684663430934014\n",
      "  (0, 1073)\t0.30413709761624086\n",
      "  (0, 2120)\t0.2684663430934014\n",
      "  (0, 1687)\t0.06619086894558779\n",
      "  (0, 1200)\t0.26383180239523174\n",
      "  (0, 1473)\t0.31679149916873117\n",
      "  (0, 1117)\t0.16480321090672406\n",
      "  (0, 4092)\t0.16167588241502212\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[0:1])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100)\n",
    "LSAData = svd.fit_transform(TfIdf_dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[   0   66  894  406  607]\n",
      " [   1  549  401  556  106]\n",
      " [   2  646  342  405 1388]\n",
      " ...\n",
      " [1477 1372  392  801  859]\n",
      " [1478  967  625  216 1146]\n",
      " [1479  788 1341  402  553]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(LSAData)\n",
    "numpy.fill_diagonal(cosineSimilarity,1.1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:5])\n",
    "print()\n",
    "print(top5similar)\n",
    "\n",
    "numpy.savetxt(\"results2.csv\", top5similar.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "plain freckles short hair boring eyes likes earrings hair parted normally probably slightly overweight but not sure about that she likes twilight she isn t married she is sad she is slightly overweight\n"
     ]
    }
   ],
   "source": [
    "print(dataSet[0])\n",
    "print()\n",
    "print(dataSet[66])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
