{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rickmackenbach/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rickmackenbach/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import string\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code for interpretability. NLTK is also used for cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, stem=True):\n",
    "    \"\"\" Tokenize text and stem words removing punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    " \n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running first results, the following sentence was found : \n",
      "\n",
      "rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad\n",
      "\n",
      "\n",
      "Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.\n"
     ]
    }
   ],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "\n",
    "print('After running first results, the following sentence was found : ')\n",
    "print()\n",
    "print(dataSet[496])\n",
    "print()\n",
    "print()\n",
    "print('Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be 'trained' to obtain accuracy measures of classification. Firstly, countVectorizer by scikitLearn (which counts all the instances of words) will run on our cleaned dataset. Afterwards TfIdf will run, in order the have the Term frequency, inverse document frequency (which will essentially put less importance on non-informative words suchs as: 'the', 'and', 'a'). Scikit-learn provides a neat function to do this in a single function, namely TdIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 3085)\t0.18016293756155294\n",
      "  (0, 1351)\t0.10858700066557808\n",
      "  (0, 3257)\t0.3001624823648848\n",
      "  (0, 2585)\t0.34043262183046635\n",
      "  (0, 2139)\t0.11306416211646501\n",
      "  (0, 4024)\t0.21854146699056293\n",
      "  (0, 1981)\t0.25581194154091114\n",
      "  (0, 3626)\t0.31679149916873117\n",
      "  (0, 1152)\t0.24315753998842082\n",
      "  (0, 4070)\t0.2684663430934014\n",
      "  (0, 1073)\t0.30413709761624086\n",
      "  (0, 2120)\t0.2684663430934014\n",
      "  (0, 1687)\t0.06619086894558779\n",
      "  (0, 1200)\t0.26383180239523174\n",
      "  (0, 1473)\t0.31679149916873117\n",
      "  (0, 1117)\t0.16480321090672406\n",
      "  (0, 4092)\t0.16167588241502212\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[0:1])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and neatly fit into an sparse array. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  1480\n",
      "The dimension of our dataset are:  (1480,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " jug ears mustache and beard and long sideburns stylish hair no laugh lines eyes are clear no drugs or alcohol confident a little overweight from double chin\n"
     ]
    }
   ],
   "source": [
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents will be provided. The first vector in the matrix represents the 'base' sentence. The vectors following are the sentences most similar to that 'base' sentence. This should be read per row. For example, the second element of the first row is most similar to the first element of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.06419899 0.01454109 ... 0.00629138 0.06862054 0.07042177]\n",
      " [0.06419899 1.         0.06743771 ... 0.04726818 0.06877843 0.04734656]\n",
      " [0.01454109 0.06743771 1.         ... 0.00565678 0.065617   0.02817174]\n",
      " ...\n",
      " [0.00629138 0.04726818 0.00565678 ... 1.         0.00482428 0.00497692]\n",
      " [0.06862054 0.06877843 0.065617   ... 0.00482428 1.         0.07985904]\n",
      " [0.07042177 0.04734656 0.02817174 ... 0.00497692 0.07985904 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Make use of SKlearn cosine similarity\n",
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "print(cosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[   0 1454   65   66  406]\n",
      " [   1  556  549 1373  944]\n",
      " [   2  342    4  288  379]\n",
      " ...\n",
      " [1477 1372  210  902  681]\n",
      " [1478  967  706  669 1084]\n",
      " [1479 1341 1144  500  773]]\n"
     ]
    }
   ],
   "source": [
    "## Adjust the cosineSimilarity matrix accordingly to sort and get results\n",
    "numpy.fill_diagonal(cosineSimilarity,1.1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:5])\n",
    "print()\n",
    "print(top5similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the cosine similarity\n",
    "\n",
    "Following the cosine metric, the first sentence in our dataSet is closest to the 1455 sentence in our data set. Let's see what they both look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 in the dataSet: \n",
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "Sentence 1455 in the dataSet: \n",
      "she is older looking and has some wrinkles she has a round face and a round nose has 2 kids has 1 grandkid not very happy likes to drink wine\n"
     ]
    }
   ],
   "source": [
    "print('Sentence 1 in the dataSet: ')\n",
    "print(dataSet[0])\n",
    "print()\n",
    "print('Sentence 1455 in the dataSet: ')\n",
    "print(dataSet[1454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 1454   65   66  406 1085]\n",
      " [   1  556  549 1373  944  144]\n",
      " [   2  342    4  288  379 1417]\n",
      " ...\n",
      " [1477 1372  210  902  681  392]\n",
      " [1478  967  706  669 1084  625]\n",
      " [1479 1341 1144  500  773  541]]\n"
     ]
    }
   ],
   "source": [
    "euclid = pairwise_distances(TfIdf_dataSet, metric='euclidean')\n",
    "euclidSorted = numpy.argsort(euclid, axis=1)\n",
    "top5SimilarEuclidean = euclidSorted[:,0:6]\n",
    "print(top5SimilarEuclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of euclidean distance, which is similar to our cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 in the dataSet: \n",
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "Sentence 1455 in the dataSet: \n",
      "she is older looking and has some wrinkles she has a round face and a round nose has 2 kids has 1 grandkid not very happy likes to drink wine\n"
     ]
    }
   ],
   "source": [
    "print('Sentence 1 in the dataSet: ')\n",
    "print(dataSet[0])\n",
    "print()\n",
    "print('Sentence 1455 in the dataSet: ')\n",
    "print(dataSet[1454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans clustering\n",
    "\n",
    "Besides finding similar documents by cosine similarity, an implementation of KMeans clustering is done in the following code. This is more meaningful, since it is known that there are 5 sentences that are equal to each other, therefore making the number of clusters to 296. Also, it allows for topic extraction, which can be interpreted as the most important words for each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480 296\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        sentences.append(text)\n",
    "        \n",
    "#sentences = sentences[0:100]\n",
    "nclusters = int(len(sentences)/5)\n",
    "\n",
    "print(len(sentences), nclusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "            #tokenizes and stems the text\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the dataset to work with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sentences(sentences, nb_of_clusters):\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                    stop_words=stopwords.words('english'),\n",
    "                                     max_df=0.9,\n",
    "                                    min_df=0.1,\n",
    "                                    lowercase=True)\n",
    "            #builds a tf-idf matrix for the sentences\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    kmeans = KMeans(n_clusters=nb_of_clusters)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    \n",
    "    kmeans_array = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    clusters = collections.defaultdict(list)\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "            clusters[label].append(i)\n",
    "\n",
    "\n",
    "    return dict(clusters), kmeans_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the KMeans in the next lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, kmeans_predict = cluster_sentences(sentences, nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for i in range(len(kmeans_predict)):\n",
    "    temp = [i]\n",
    "    for j in range(1,len(kmeans_predict)):\n",
    "        jl = [j]\n",
    "        if kmeans_predict[i]==kmeans_predict[j] and j not in temp:\n",
    "            temp += [j]\n",
    "    output.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 158, 353, 685, 844]\n",
      "[1, 176, 1048, 1405, 176]\n",
      "[2, 305, 601, 1149, 305]\n",
      "[3, 48, 352, 623, 636]\n",
      "[4, 196, 809, 1050, 196]\n",
      "[5, 336, 567, 749, 930]\n",
      "[6, 171, 799, 846, 962]\n",
      "[7, 596, 642, 652, 699]\n",
      "[8, 562, 562, 562, 562]\n",
      "[9, 849, 946, 1321, 849]\n",
      "[10, 30, 59, 74, 477]\n"
     ]
    }
   ],
   "source": [
    "for line in output:\n",
    "    if len(line) > 5:\n",
    "        index = output.index(line)\n",
    "        output[index] = line[0:5]\n",
    "    if len(line) > 1  and len(line) < 5:\n",
    "        line += [line[1]]*(5-len(line))\n",
    "    if len(line) == 1:\n",
    "        line += [line[0]]*4\n",
    "\n",
    "for line in output[0:11]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the KMeans results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence:\n",
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "The 159th sentence:\n",
      "f physical appearance that are fairly constant during an encounter i e excluding emotional expression are partly biologically their emotional state as shown by the drabness or brightness of their clothes life is too short for shitty wine you if you can feel people pulling away you should ask them about it\n"
     ]
    }
   ],
   "source": [
    "print('The first sentence:')\n",
    "print(dataSet[0])\n",
    "print()\n",
    "print('The 159th sentence:')\n",
    "print(dataSet[158])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be necessary to run the KMeans two or three times to get similar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
