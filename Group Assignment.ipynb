{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code to improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "#print('After running first results, the following sentence was found : ')\n",
    "##line 496 is the weird one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.save(\"descriptions_cleaned_array.npy\",dataSet)\n",
    "#dataSet = numpy.load(\"descriptions_cleaned_array.npy\")\n",
    "dataSet = numpy.load(\"coco_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and neatly fit into an array. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  50\n",
      "The dimension of our dataset are:  (50,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " a child holding a flowered umbrella and petting a yak\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " a young man holding an umbrella next to a herd of cattle\n"
     ]
    }
   ],
   "source": [
    "dataSet = dataSet[0:50]\n",
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be trained to obtain accuracy measures of classification. Firstly, countVectorizer by scikitLearn (which counts all the instances of words) will run on our cleaned dataset. Afterwards TfIdf will run, in order the have the Term frequency, inverse document frequency (which will essentially put less importance on non-informative words suchs as: 'the', 'and', 'a'). Scikit-learn provides a neat function to do this in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
      "  (0, 24)\t0.43611951897636925\n",
      "  (0, 64)\t0.28136432388711935\n",
      "  (0, 48)\t0.43611951897636925\n",
      "  (0, 155)\t0.32308280994485267\n",
      "  (0, 2)\t0.23590395467851702\n",
      "  (0, 110)\t0.43611951897636925\n",
      "  (0, 169)\t0.43611951897636925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[0:1])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents will be provided. The first vector in the matrix represents the 'base' sentence. The vectors following are the sentences most similar to that 'base' sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  4  1  2 12  3]\n",
      " [ 1  2  4  3 12 10]\n",
      " [ 2  3  1  4 12  0]\n",
      " [ 3  2  1  4 18  9]\n",
      " [ 4  1  2  3 10  0]\n",
      " [ 5  6  7  8 43 17]\n",
      " [ 6  5  7  8 28 39]\n",
      " [ 7  5  6  8  9 17]\n",
      " [ 8  9 43  5 34  6]\n",
      " [ 9 18  8 12  3 26]\n",
      " [10 11 12 13  4  1]\n",
      " [11 10 49 26 48 12]\n",
      " [12 13 14  9  1 26]\n",
      " [13 12 14 47 10 46]\n",
      " [14 12 13 21 26 10]\n",
      " [15 30 32 35 16 48]\n",
      " [16 15 18 49 45 48]\n",
      " [17 19 43 34 16  5]\n",
      " [18  9 45 44 15 26]\n",
      " [19 17 18 31 15 30]\n",
      " [20 23 21 22 37 15]\n",
      " [21 20 23 14 41 22]\n",
      " [22 23 20 21 24 43]\n",
      " [23 22 20 21 24 43]\n",
      " [24 46 23 36 22 35]\n",
      " [25 29 27 18 15 32]\n",
      " [26 29 27 28 18 12]\n",
      " [27 26 28 25 36 29]\n",
      " [28 26 29 27 39 36]\n",
      " [29 26 28 25 27 15]\n",
      " [30 32 15 44 35 40]\n",
      " [31 18 19  9 15 26]\n",
      " [32 30 15 35 40 44]\n",
      " [33 43 34 15 30 24]\n",
      " [34 43 44 15 40 33]\n",
      " [35 36 37 15 32 39]\n",
      " [36 35 39 37 38 28]\n",
      " [37 35 36 39 20  1]\n",
      " [38 39 36 28 27 46]\n",
      " [39 38 36 28 35 37]\n",
      " [40 30 32 43 44 34]\n",
      " [41 49 47 21 40  3]\n",
      " [42 44 43 30 40 32]\n",
      " [43 34 44 42 15 40]\n",
      " [44 43 30 18 15 34]\n",
      " [45 46 47 49 18 48]\n",
      " [46 47 45 48 49 13]\n",
      " [47 46 45 48 49 13]\n",
      " [48 46 49 47 15 45]\n",
      " [49 45 48 46 47 41]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "numpy.fill_diagonal(cosineSimilarity,1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:6])\n",
    "print(top5similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the cosine similarity\n",
    "\n",
    "Following the cosine metric, the first sentence in our dataSet is closest to the 1455 sentence in our data set. Let's see what they both look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Sentence 1 in the dataSet: ')\n",
    "# print(dataSet[0])\n",
    "# print()\n",
    "# print('Sentence 1455 in the dataSet: ')\n",
    "# print(dataSet[1454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Besides finding similar documents by cosine similarity, an implementation of KMeans clustering is done in the following code. This is more meaningful, since it is known that there are 5 sentences that are equal to each other, therefore making the number of clusters to 296. Also, it allows for topic extraction, which can be interpreted as the most important words for each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = sklearn.cluster.KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 5 5 0 0 0 4 7 9 3 6 6 6 9 7 4 7 7 8 8 8 8 8 7 3 3 3 3 9 7 9 4 4 2 2\n",
      " 2 2 2 9 9 4 4 4 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "KmeansFit = KMeans.fit(TfIdf_dataSet)\n",
    "print(KmeansFit.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [4 7 3 7 9 9 5 9 7 4]\n",
      " [7 9 1 2 7 6 3 4 4 7]\n",
      " [2 3 9 1 0 7 1 3 2 5]\n",
      " [1 2 8 6 8 1 7 1 9 1]\n",
      " [8 5 5 9 1 2 9 5 5 2]\n",
      " [3 4 7 5 2 3 8 8 0 3]\n",
      " [5 6 0 8 5 8 4 0 1 8]\n",
      " [6 0 4 0 6 4 0 2 6 6]\n",
      " [9 8 6 4 3 0 2 6 3 0]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarityK = sklearn.metrics.pairwise.cosine_similarity(KmeansFit.cluster_centers_)\n",
    "numpy.fill_diagonal(cosineSimilarityK,1)\n",
    "cosineSimilaritySortedK = numpy.argsort((-1*(cosineSimilarityK)),axis=0)\n",
    "print(cosineSimilaritySortedK)\n",
    "top5similarK = (cosineSimilaritySortedK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save results to results file\n",
    "numpy.savetxt(\"results.csv\",top5similarK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " appliances\n",
      " kitchen\n",
      " with\n",
      " and\n",
      " white\n",
      "Cluster 1:\n",
      " helmet\n",
      " toilet\n",
      " red\n",
      " yellow\n",
      " it\n",
      "Cluster 2:\n",
      " parking\n",
      " cars\n",
      " meters\n",
      " street\n",
      " meter\n",
      "Cluster 3:\n",
      " riding\n",
      " street\n",
      " bike\n",
      " her\n",
      " down\n",
      "Cluster 4:\n",
      " bathroom\n",
      " bathtub\n",
      " pedestal\n",
      " sink\n",
      " claw\n",
      "Cluster 5:\n",
      " umbrella\n",
      " holding\n",
      " an\n",
      " boy\n",
      " young\n",
      "Cluster 6:\n",
      " cat\n",
      " girl\n",
      " small\n",
      " holding\n",
      " shirt\n",
      "Cluster 7:\n",
      " the\n",
      " bathroom\n",
      " is\n",
      " door\n",
      " to\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = KmeansFit.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(8):\n",
    "    print (\"Cluster %d:\" % i,)\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print (' %s' % terms[ind],)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "To actually get a better estimation of our text similarity, several tests are performed on a test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
