{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code to improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weird sentences were found, such as: \n",
      "\n",
      "rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad\n",
      "\n",
      "However, it will remain in the dataset for evaluating results\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
<<<<<<< HEAD
    "grossData = numpy.array(descriptions)\n",
    "print('Weird sentences were found, such as: ')\n",
    "print()\n",
    "print(grossData[496])\n",
    "print()\n",
    "print('However, it will remain in the dataset for evaluating results')\n",
    "f.close()\n",
    "dataSet = grossData"
=======
    "dataSet = numpy.array(descriptions)\n",
    "#print('After running first results, the following sentence was found : ')\n",
    "##line 496 is the weird one\n"
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.save(\"descriptions_cleaned_array.npy\",dataSet)\n",
    "#dataSet = numpy.load(\"descriptions_cleaned_array.npy\")\n",
    "dataSet = numpy.load(\"coco_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and neatly fit into an array. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  25000\n",
      "The dimension of our dataset are:  (25000,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " a child holding a flowered umbrella and petting a yak\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " a young man holding an umbrella next to a herd of cattle\n"
     ]
    }
   ],
   "source": [
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])\n",
    "dataSet = dataSet[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be trained to obtain accuracy measures of classification. Firstly, countVectorizer by scikitLearn (which counts all the instances of words) will run on our cleaned dataset. Afterwards TfIdf will run, in order the have the Term frequency, inverse document frequency (which will essentially put less importance on non-informative words suchs as: 'the', 'and', 'a'). Scikit-learn provides a neat function to do this in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like: \n",
      "\n",
<<<<<<< HEAD
      "  (0, 24)\t0.43611951897636925\n",
      "  (0, 64)\t0.28136432388711935\n",
      "  (0, 48)\t0.43611951897636925\n",
      "  (0, 155)\t0.32308280994485267\n",
      "  (0, 2)\t0.23590395467851702\n",
      "  (0, 110)\t0.43611951897636925\n",
      "  (0, 169)\t0.43611951897636925\n"
=======
      "  (0, 228)\t0.0999230235196548\n",
      "  (0, 2737)\t0.16083047896043273\n",
      "  (0, 1776)\t0.06254106376813398\n",
      "  (0, 2119)\t0.3161753448886334\n",
      "  (0, 1247)\t0.14464356086569088\n",
      "  (0, 2538)\t0.19064057405466286\n",
      "  (0, 419)\t0.18076247495377387\n",
      "  (0, 2292)\t0.1401975901762004\n",
      "  (0, 3445)\t0.21120111824703844\n",
      "  (0, 3749)\t0.2492839547283478\n",
      "  (0, 2604)\t0.3123113475808705\n",
      "  (0, 2188)\t0.2417063139284673\n",
      "  (0, 2262)\t0.264108063599575\n",
      "  (0, 1420)\t0.07045478691841404\n",
      "  (0, 286)\t0.11292286212515924\n",
      "  (0, 772)\t0.21289776024949109\n",
      "  (0, 1217)\t0.2324320311261342\n",
      "  (0, 2694)\t0.1235469355267728\n",
      "  (0, 180)\t0.27051486760744353\n",
      "  (0, 878)\t0.20649095624162253\n",
      "  (0, 2276)\t0.14173141580413545\n",
      "  (0, 1605)\t0.1515561961580996\n",
      "  (0, 1191)\t0.3161753448886334\n",
      "  (0, 740)\t0.1644468421153148\n"
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \")\n",
    "print()\n",
    "print(TfIdf_dataSet[0:1])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
<<<<<<< HEAD
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents per sentences will be provided."
=======
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents will be provided."
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 14,
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  4  1  2 12  3]\n",
      " [ 1  2  4  3 12 10]\n",
      " [ 2  3  1  4 12  0]\n",
      " [ 3  2  1  4 18  9]\n",
      " [ 4  1  2  3 10  0]\n",
      " [ 5  6  7  8 43 17]\n",
      " [ 6  5  7  8 28 39]\n",
      " [ 7  5  6  8  9 17]\n",
      " [ 8  9 43  5 34  6]\n",
      " [ 9 18  8 12  3 26]\n",
      " [10 11 12 13  4  1]\n",
      " [11 10 49 26 48 12]\n",
      " [12 13 14  9  1 26]\n",
      " [13 12 14 47 10 46]\n",
      " [14 12 13 21 26 10]\n",
      " [15 30 32 35 16 48]\n",
      " [16 15 18 49 45 48]\n",
      " [17 19 43 34 16  5]\n",
      " [18  9 45 44 15 26]\n",
      " [19 17 18 31 15 30]\n",
      " [20 23 21 22 37 15]\n",
      " [21 20 23 14 41 22]\n",
      " [22 23 20 21 24 43]\n",
      " [23 22 20 21 24 43]\n",
      " [24 46 23 36 22 35]\n",
      " [25 29 27 18 15 32]\n",
      " [26 29 27 28 18 12]\n",
      " [27 26 28 25 36 29]\n",
      " [28 26 29 27 39 36]\n",
      " [29 26 28 25 27 15]\n",
      " [30 32 15 44 35 40]\n",
      " [31 18 19  9 15 26]\n",
      " [32 30 15 35 40 44]\n",
      " [33 43 34 15 30 24]\n",
      " [34 43 44 15 40 33]\n",
      " [35 36 37 15 32 39]\n",
      " [36 35 39 37 38 28]\n",
      " [37 35 36 39 20  1]\n",
      " [38 39 36 28 27 46]\n",
      " [39 38 36 28 35 37]\n",
      " [40 30 32 43 44 34]\n",
      " [41 49 47 21 40  3]\n",
      " [42 44 43 30 40 32]\n",
      " [43 34 44 42 15 40]\n",
      " [44 43 30 18 15 34]\n",
      " [45 46 47 49 18 48]\n",
      " [46 47 45 48 49 13]\n",
      " [47 46 45 48 49 13]\n",
      " [48 46 49 47 15 45]\n",
      " [49 45 48 46 47 41]]\n"
     ]
    }
   ],
   "source": [
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "numpy.fill_diagonal(cosineSimilarity,1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:6])\n",
    "print(top5similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the cosine similarity\n",
    "\n",
    "Following the cosine metric, the first sentence in our dataSet is closest to the 1455 sentence in our data set. Let's see what they both look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Sentence 1 in the dataSet: ')\n",
    "# print(dataSet[0])\n",
    "# print()\n",
    "# print('Sentence 1455 in the dataSet: ')\n",
    "# print(dataSet[1454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Besides finding similar documents by cosine similarity, an implementation of KMeans clustering is done in the following code. This is more meaningful, since it is known that there are 5 sentences that are equal to each other, therefore making the number of clusters to 296. Also, it allows for topic extraction, which can be interpreted as the most important words for each cluster. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 22,
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans = sklearn.cluster.KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 23,
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 5 5 5 5 7 7 7 7 7 7 2 2 5 6 2 3 3 3 3 3 0 0 0 0 0 2 2 2 9 8 4 4\n",
      " 4 4 4 8 8 8 8 8 6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "KmeansFit = KMeans.fit(TfIdf_dataSet)\n",
    "print(KmeansFit.labels_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 24,
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [4 7 8 2 0 8 2 1 2 8]\n",
      " [6 2 6 5 2 2 7 6 5 2]\n",
      " [7 6 4 4 6 4 0 0 9 3]\n",
      " [2 0 0 8 5 3 8 2 6 5]\n",
      " [1 4 5 1 3 6 4 5 3 6]\n",
      " [3 8 1 6 1 7 1 3 1 0]\n",
      " [5 3 7 7 8 1 5 8 4 1]\n",
      " [8 5 3 0 7 0 3 4 7 4]\n",
      " [9 9 9 9 9 9 9 9 0 7]]\n"
=======
      "[[2912 2913 2914 2915 2916]\n",
      " [2898 2899 2900 2901 2902]\n",
      " [2878 2879 2880 2881 2882]\n",
      " ...\n",
      " [2905 2906 2907 2908 2909]\n",
      " [2911 2912 2913 2914 2915]\n",
      " [2889 2890 2891 2892 2893]]\n"
>>>>>>> 7dbda746d463ae2d0cba9a9920d525b90c09d3ad
     ]
    }
   ],
   "source": [
    "cosineSimilarityK = sklearn.metrics.pairwise.cosine_similarity(KmeansFit.cluster_centers_)\n",
    "numpy.fill_diagonal(cosineSimilarityK,1)\n",
    "cosineSimilaritySortedK = numpy.argsort((-1*(cosineSimilarityK)),axis=0)\n",
    "print(cosineSimilaritySortedK)\n",
    "top5similarK = (cosineSimilaritySortedK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">I think we still need to order by the first column's value so that we get the lines in a row. Then we save as a CSV as noted in code line below</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save results to results file\n",
    "numpy.savetxt(\"results.csv\",top5SimilarKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " riding\n",
      " street\n",
      " bike\n",
      " on\n",
      " down\n",
      "Cluster 1:\n",
      " umbrella\n",
      " holding\n",
      " an\n",
      " boy\n",
      " young\n",
      "Cluster 2:\n",
      " to\n",
      " sitting\n",
      " bathroom\n",
      " sink\n",
      " bath\n",
      "Cluster 3:\n",
      " two\n",
      " sinks\n",
      " mirrors\n",
      " steel\n",
      " stainless\n",
      "Cluster 4:\n",
      " parking\n",
      " cars\n",
      " meters\n",
      " street\n",
      " meter\n",
      "Cluster 5:\n",
      " kitchen\n",
      " appliances\n",
      " with\n",
      " and\n",
      " wall\n",
      "Cluster 6:\n",
      " toilet\n",
      " helmet\n",
      " red\n",
      " yellow\n",
      " it\n",
      "Cluster 7:\n",
      " girl\n",
      " small\n",
      " cat\n",
      " holding\n",
      " kitten\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = KmeansFit.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(8):\n",
    "    print (\"Cluster %d:\" % i,)\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print (' %s' % terms[ind],)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "To actually get a better estimation of our text similarity, several tests are performed on a test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
