{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of nearly any machine learning analysis project is data cleaning. This is done in order to allow a larger variety of models to work with a predictable input, such that exceptions (in this case special characters such as quotation marks, '[comma]' and others) will not cause any disturbance in the model. The following code loads the data, 'cleans' it, and afterwards sets the entire cleaned data in an array. Comments are added in the code for interpretability. NLTK is also used for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After running first results, the following sentence was found : \n",
      "\n",
      "rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewadrwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad rwevfderdcerfadwerfdewad\n",
      "\n",
      "\n",
      "Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.\n"
     ]
    }
   ],
   "source": [
    "## Set an empty list variable\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "with open('descriptions.txt', encoding = \"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.lower()                                       ## Lowercase all characters\n",
    "        text = text.replace(\"[comma]\",\" \")                        ## Replace [commas] with empty space\n",
    "        for ch in text:\n",
    "            if ch < \"0\" or (ch < \"a\" and ch > \"9\") or ch > \"z\":   ## The cleaning operation happens here, remove all special characters\n",
    "                text = text.replace(ch,\" \")\n",
    "        text = ' '.join(text.split())                             ## Remove double spacing from sentences\n",
    "        descriptions.append(text)\n",
    "dataSet = numpy.array(descriptions)\n",
    "\n",
    "print('After running first results, the following sentence was found : ')\n",
    "print()\n",
    "print(dataSet[496])\n",
    "print()\n",
    "print()\n",
    "print('Although this sentence is not meaningful, it will remain in the dataset in order to have consistent results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation - TfIdf Vectorizer\n",
    "\n",
    "Since the input vector now is 'clean', different representations can be made, which in turn can then be 'trained' to obtain accuracy measures of classification. Used TfIdf vectorizer in order the have the Term frequency, inverse document frequency (which will essentially put less importance on non-informative words suchs as: 'the', 'and', 'a'). We included the stop_words parameter which also strips out unimportant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What our Tf-Idf looks like:  \n",
      "\n",
      "  (0, 3085)\t0.18016293756155294\n",
      "  (0, 1351)\t0.10858700066557808\n",
      "  (0, 3257)\t0.3001624823648848\n",
      "  (0, 2585)\t0.34043262183046635\n",
      "  (0, 2139)\t0.11306416211646501\n",
      "  (0, 4024)\t0.21854146699056293\n",
      "  (0, 1981)\t0.25581194154091114\n",
      "  (0, 3626)\t0.31679149916873117\n",
      "  (0, 1152)\t0.24315753998842082\n",
      "  (0, 4070)\t0.2684663430934014\n",
      "  (0, 1073)\t0.30413709761624086\n",
      "  (0, 2120)\t0.2684663430934014\n",
      "  (0, 1687)\t0.06619086894558779\n",
      "  (0, 1200)\t0.26383180239523174\n",
      "  (0, 1473)\t0.31679149916873117\n",
      "  (0, 1117)\t0.16480321090672406\n",
      "  (0, 4092)\t0.16167588241502212\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "TfIdf_dataSet = vectorizer.fit_transform(dataSet)\n",
    "print(\"What our Tf-Idf looks like: \",\"\\n\")\n",
    "print(TfIdf_dataSet[0:1])\n",
    "\n",
    "vectorVocab = vectorizer._validate_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and neatly fit into an sparse array. Some basic information about the cleaned array will be provided in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our data set:  1480\n",
      "The dimension of our dataset are:  (1480,)\n",
      "\n",
      "\n",
      "-- 0th element of our dataSet -- \n",
      " round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "\n",
      "-- 1st element of our dataSet -- \n",
      " jug ears mustache and beard and long sideburns stylish hair no laugh lines eyes are clear no drugs or alcohol confident a little overweight from double chin\n"
     ]
    }
   ],
   "source": [
    "print('The size of our data set: ', dataSet.size)\n",
    "print('The dimension of our dataset are: ', dataSet.shape)\n",
    "print('\\n')\n",
    "print('-- 0th element of our dataSet --', '\\n', dataSet[0])\n",
    "print('\\n')\n",
    "print('-- 1st element of our dataSet --', '\\n', dataSet[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Measure - Cosine similarity\n",
    "\n",
    "Now we can safely compute the distance between each document. After sorting, the most similar top 5 documents will be provided. The first vector in the matrix represents the 'base' sentence. The vectors following are the sentences most similar to that 'base' sentence. This should be read per row. For example, the second element of the first row is most similar to the first element of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.06419899 0.01454109 ... 0.00629138 0.06862054 0.07042177]\n",
      " [0.06419899 1.         0.06743771 ... 0.04726818 0.06877843 0.04734656]\n",
      " [0.01454109 0.06743771 1.         ... 0.00565678 0.065617   0.02817174]\n",
      " ...\n",
      " [0.00629138 0.04726818 0.00565678 ... 1.         0.00482428 0.00497692]\n",
      " [0.06862054 0.06877843 0.065617   ... 0.00482428 1.         0.07985904]\n",
      " [0.07042177 0.04734656 0.02817174 ... 0.00497692 0.07985904 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Make use of SKlearn cosine similarity\n",
    "cosineSimilarity = sklearn.metrics.pairwise.cosine_similarity(TfIdf_dataSet)\n",
    "print(cosineSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[   0 1454   65   66  406]\n",
      " [   1  556  549 1373  944]\n",
      " [   2  342    4  288  379]\n",
      " ...\n",
      " [1477 1372  210  902  681]\n",
      " [1478  967  706  669 1084]\n",
      " [1479 1341 1144  500  773]]\n"
     ]
    }
   ],
   "source": [
    "## Adjust the cosineSimilarity matrix accordingly to sort and get results\n",
    "numpy.fill_diagonal(cosineSimilarity,1.1)\n",
    "cosineSimilaritySorted = numpy.argsort((-1*(cosineSimilarity)),axis=1)\n",
    "top5similar = (cosineSimilaritySorted[:,0:5])\n",
    "print()\n",
    "print(top5similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the cosine similarity\n",
    "\n",
    "Following the cosine metric, the first sentence in our dataSet is closest to the 1455 sentence in our data set. Let's see what they both look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 in the dataSet: \n",
      "round face short and overweight likes to wear jeans and sweaters drinks wine at dinner short liberal overweight short hair eats at whole foods does not work our very much\n",
      "\n",
      "Sentence 1455 in the dataSet: \n",
      "she is older looking and has some wrinkles she has a round face and a round nose has 2 kids has 1 grandkid not very happy likes to drink wine\n"
     ]
    }
   ],
   "source": [
    "print('Sentence 1 in the dataSet: ')\n",
    "print(dataSet[0])\n",
    "print()\n",
    "print('Sentence 1455 in the dataSet: ')\n",
    "print(dataSet[1454])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savetxt(\"results.csv\", top5similar.astype(int), fmt='%i', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
